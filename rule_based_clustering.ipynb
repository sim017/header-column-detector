{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335cb2a-aeb4-4359-a981-618340560c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f09f5-b78f-441c-8ba6-8fa949f19dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_file_desc = pd.read_csv('header_information.csv')\n",
    "header_file_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e04bcb-a939-4653-86fe-f51fa114f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preProcess:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def text_process(self, text):\n",
    "        '''\n",
    "        Takes in a string of text, then performs the following:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all stopwords\n",
    "        3. Return the cleaned text as a list of words\n",
    "        4. Remove words\n",
    "        '''\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        nopunc = [char for char in text if char not in string.punctuation]\n",
    "        nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n",
    "        nopunc =  [word for word in nopunc.split() if word not in stopwords.words('english')]\n",
    "        return [stemmer.lemmatize(word) for word in nopunc]\n",
    "    \n",
    "\n",
    "    def pre_process(self, data_row):\n",
    "        preprocess_data_row = []\n",
    "        for dataelem in data_row:\n",
    "            preprocess_data_row.append(self.text_process(dataelem))\n",
    "        return preprocess_data_row\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a621dcf-d9ff-47fa-a863-8cee4212b062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self):\n",
    "        self.preprocess = preProcess()\n",
    "    def isNumeric(self, char):\n",
    "        import re\n",
    "        regex = re.findall(r\"^[-+]?(?:\\d*\\.\\d+|\\d+$)\", char)\n",
    "        if regex:\n",
    "            return True\n",
    "        for c in char:\n",
    "            if not c.isdigit():\n",
    "                return False\n",
    "        return True\n",
    "    def isOnlyAlpha(self, char):\n",
    "        regex = re.findall(r\"[A-Za-z]+$\", char)\n",
    "        if regex:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def isBlank(self, each_element):\n",
    "        if not each_element or each_element == 'UNKNOWN' or each_element.isspace() or each_element == 'NULL' or each_element == ' ' or each_element == '' or each_element == None or each_element == \"\" or each_element ==\" \":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def specialCharacters(self,char):\n",
    "        import re\n",
    "        regex = re.findall(r'[\\w]',char)\n",
    "        if regex:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def isDate(self, string, fuzzy=False):\n",
    "        \"\"\"\n",
    "        Return whether the string can be interpreted as a date.\n",
    "\n",
    "        :param string: str, string to check for date\n",
    "        :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "        \"\"\"\n",
    "        try: \n",
    "            parse(string, fuzzy=fuzzy)\n",
    "            return True\n",
    "\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def noSpecialCharacters(self, char):\n",
    "        noSpecialChars = 0\n",
    "        noSpecialChars =  len(char) - len( re.findall('[\\w]', char) ) + len(re.findall('[-_]+',char))\n",
    "        return noSpecialChars\n",
    "    \n",
    "    def tokenize( self, string, separator = ',', quote = '\"' ):\n",
    "        \"\"\"\n",
    "        Split a comma separated string into a List of strings.\n",
    "\n",
    "        Separator characters inside the quotes are ignored.\n",
    "\n",
    "        :param string: A string to be split into chunks\n",
    "        :param separator: A separator character\n",
    "        :param quote: A character to define beginning and end of the quoted string\n",
    "        :return: A list of strings, one element for every chunk\n",
    "        \"\"\"\n",
    "        comma_separated_list = []\n",
    "\n",
    "        chunk = ''\n",
    "        in_quotes = False\n",
    "\n",
    "        for character in string:\n",
    "            if character == separator and not in_quotes:\n",
    "                comma_separated_list.append(chunk)\n",
    "                chunk = ''\n",
    "\n",
    "            else:\n",
    "                chunk += character\n",
    "                if character == quote:\n",
    "                    in_quotes = False if in_quotes else True\n",
    "        # print(chunk)\n",
    "        # print(chunk.replace('\"',''))\n",
    "        \n",
    "        comma_separated_list.append(chunk.replace('\"',''))\n",
    "        return comma_separated_list\n",
    "    \n",
    "    def featureList(self, file_name):\n",
    "        file = open(file_name, 'r')\n",
    "        lines = file.readlines()\n",
    "        feature_list = []\n",
    "        feature_list_vector = []\n",
    "        for line in lines:\n",
    "            feature = {}\n",
    "            # each_row_elements = [ '{}'.format(x.replace(\"\",'')) for x in list(csv.reader([line], delimiter=',', quotechar='\"'))[0] ]\n",
    "            # each_row_elements = line.split(r\",(?=(?:[^\\\"']*[\\\"'][^\\\"']*[\\\"'])*[^\\\"']*$)\")\n",
    "            each_row_elements = self.tokenize(line)\n",
    "            num_numerics = 0\n",
    "            num_alphs = 0\n",
    "            num_null = 0\n",
    "            # print(each_row_elements)\n",
    "            for each_element in each_row_elements:\n",
    "                \n",
    "                \n",
    "                if self.isBlank(each_element):\n",
    "                    num_null = num_null + 1\n",
    "                elif not self.isNumeric(each_element):\n",
    "                    num_alphs = num_alphs + 1\n",
    "                    # print('alphs',each_element)\n",
    "                    each_element = self.preprocess.text_process(each_element)\n",
    "                else:\n",
    "                    # print('numeric',each_element)\n",
    "                    num_numerics = num_numerics + 1\n",
    "#             if num_null == 0:\n",
    "#                 if num_numerics == 0:\n",
    "                    \n",
    "#                     print('header line')\n",
    "#                     print(line)\n",
    "#             else :\n",
    "#                 pass\n",
    "            feature['num_numerics'] = num_numerics\n",
    "            feature['num_null'] = num_null\n",
    "            feature['num_alphs'] = num_alphs\n",
    "            feature_list.append(feature)\n",
    "            if num_numerics == 0 and num_null == 0 and num_alphs == 0:\n",
    "                print(\"tha values seem to be empty\")\n",
    "            else:\n",
    "                feature_list_vector.append([num_numerics, num_null, num_alphs])\n",
    "        df_feature_list = pd.DataFrame(columns = ['num_numerics', 'num_null', 'num_alphs'])\n",
    "        df_feature_list['num_numerics'] = [feature[0] for feature in feature_list_vector]\n",
    "        df_feature_list['num_null'] = [feature[1] for feature in feature_list_vector]\n",
    "        df_feature_list['num_alphs'] = [feature[2] for feature in feature_list_vector]\n",
    "        return lines,feature_list, feature_list_vector, df_feature_list \n",
    "    \n",
    "    \n",
    "    def rule_based_classification(self, file):\n",
    "        lines, feature_list, feature_list_vector, df_feature_list = self.featureList(file)\n",
    "        \n",
    "        #rule 2 -> check for the no null values in the header file and no numerics\n",
    "    \n",
    "        idx = df_feature_list.index[(df_feature_list['num_numerics'] == 0) & (df_feature_list['num_null'] == 0)].to_list()\n",
    "       \n",
    "        #rule 3 -> incase more than one elements in the list is given then apply the classification algorithm for the same\n",
    "        print(idx)\n",
    "        if len(idx) > 1:\n",
    "            \n",
    "            pred_indices = []\n",
    "            \n",
    "            # each_row_elements = [ '{}'.format(x.replace(\"\",'')) for x in list(csv.reader([lines[idx[0]]], delimiter=',', quotechar='\"'))[0] ]\n",
    "            each_row_elements = self.tokenize(lines[idx[0]])\n",
    "            column_len = len(each_row_elements)\n",
    "            # print(\"column len\", column_len)\n",
    "            \n",
    "            data = []\n",
    "            for i,index in enumerate(idx):\n",
    "                \n",
    "                data_row = lines[index]\n",
    "                # print(data_row)\n",
    "                # data_row = [ '{}'.format(x.replace(\"\",'')) for x in list(csv.reader([data_row], delimiter=',', quotechar='\"'))[0] ]\n",
    "                # print(data_row)\n",
    "                data_row = self.tokenize(data_row)\n",
    "                data.append(data_row)\n",
    "                print(data_row)\n",
    "                # print(data_row)\n",
    "                # df_text.loc[i] = data_row\n",
    "                # df_sub_feature = pd.DataFrame(columns = ['num_special_chars', 'data_len', 'bool_numeric', 'bool_alpha'])\n",
    "                # for data in data_row:\n",
    "                #     num_special_chars = self.noSpecialCharacters(data)\n",
    "                #     data_len = len(data)\n",
    "                #     bool_numeric = 1 if self.isNumeric(data) else 0\n",
    "                #     bool_alpha = 1 if self.isOnlyAlpha(data) else 0\n",
    "                #     sub_feature_vector.append([num_special_chars, data_len, bool_numeric, bool_alpha])\n",
    "                # print(sub_feature_vector)\n",
    "                # kmeans.fit(sub_feature_vector)\n",
    "                # identified_clusters = kmeans.fit_predict(sub_feature_vector)\n",
    "                # print(identified_clusters)\n",
    "            df_text = pd.DataFrame(data, columns = [ x for x in range(column_len)])\n",
    "            # print(df_text)\n",
    "            for i in range(column_len):\n",
    "                sub_feature_vector = []\n",
    "                data_row = df_text[i]\n",
    "                print(data_row)\n",
    "                for data in data_row:\n",
    "                    if data is not None:\n",
    "                        data = data.strip('\\\"')\n",
    "                        num_special_chars = self.noSpecialCharacters(data)\n",
    "                        data_len = len(data)\n",
    "                        bool_numeric = 1 if self.isNumeric(data) else 0\n",
    "                        bool_alpha = 1 if self.isOnlyAlpha(data) else 0\n",
    "                        bool_date = 1 if self.isDate(data) else 0\n",
    "                        sub_feature_vector.append([num_special_chars, data_len, bool_numeric, bool_alpha, bool_date])\n",
    "                    \n",
    "                print(sub_feature_vector)\n",
    "                scaler = StandardScaler()\n",
    "                sub_feature_vector = scaler.fit_transform(sub_feature_vector)\n",
    "                \n",
    "                kmeans.fit(sub_feature_vector)\n",
    "                identified_clusters = kmeans.fit_predict(sub_feature_vector)\n",
    "                print(identified_clusters)\n",
    "                map_index = {}\n",
    "                \n",
    "                count_arr = np.bincount(identified_clusters)\n",
    "                if len(count_arr) == 1:\n",
    "                    count_arr = np.append(count_arr, [0])\n",
    "                if count_arr[0] > 1 and count_arr[1]>1:\n",
    "                    pass\n",
    "                elif count_arr[0] == 1 and count_arr[1] > 1:\n",
    "                    for i in range(len(identified_clusters)):\n",
    "                        if identified_clusters[i]==0:\n",
    "                            if i in map_index.keys():\n",
    "                                map_index[i] = map_index[i] + 1\n",
    "                            else:\n",
    "                                map_index[i] = 0\n",
    "                elif count_arr[0] > 1 and count_arr[1] == 1:\n",
    "                    for i in range(len(identified_clusters)):\n",
    "                        if identified_clusters[i]== 1:\n",
    "                            if i in map_index.keys():\n",
    "                                map_index[i] = map_index[i] + 1\n",
    "                            else:\n",
    "                                map_index[i] = 0\n",
    "                else:\n",
    "                    pass\n",
    "                max_occ = -1\n",
    "                max_index = 0\n",
    "                for i in range(len(identified_clusters)):\n",
    "                    \n",
    "                    if i in map_index.keys() and map_index[i] > max_occ :\n",
    "                        max_occ = map_index[i]\n",
    "                        max_index = i\n",
    "                    \n",
    "                \n",
    "                print(\"max_index\",max_index)\n",
    "                value = self.isHeader(\n",
    "                \n",
    "                \n",
    "        def isHeader(self, df):\n",
    "            n=5, th=0.9\n",
    "            # df1 = pd.read_csv(path, header='infer', nrows=n)\n",
    "            # df2 = pd.read_csv(path, header=None, nrows=n)\n",
    "            df1 = pd.DataFrame(data = df.values, header = 'infer')\n",
    "            df2 = pd.DataFrame(data = df.values, header = None)\n",
    "            sim = (df1.dtypes.values == df2.dtypes.values).mean()\n",
    "            return value = 'infer' if sim < th else None\n",
    "            \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b03293-08fb-44c7-9308-4dfe53c2190a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b75f5-5485-451a-a0eb-3d85e864bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyparsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb46f7a0-c413-4cdc-8986-145832071b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ x for x in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611175b-5055-4c36-90b0-d26e73cff200",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyparsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb158d-95fd-4f1b-91c1-efbb6bb696ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([0,0])\n",
    "count_arr = np.bincount(arr)\n",
    "print(count_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441923f5-17a3-48fb-b70e-8dc7e30bf5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class oneSVMModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def getModel(self):\n",
    "        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "        return clf\n",
    "model = oneSVMModel().getModel()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e2de2-f122-41e0-8f50-fa6dd98bb8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(2,max_iter=400, algorithm = 'auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e88c82-20d0-49de-8e4d-20886acfe665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file_name = 'header_information.csv'\n",
    "df = pd.read_csv(csv_file_name)\n",
    "file_names = df['file_name']\n",
    "headers = df['has_header']\n",
    "df_header = pd.DataFrame(columns = ['file_name','num_numerics','num_null','num_alphs','header_present'])\n",
    "for file_name, header in zip(file_names, headers):\n",
    "    #rule 1\n",
    "    if header == 'yes':\n",
    "        file = 'data/'+file_name\n",
    "        print(file)\n",
    "        Features().rule_based_classification(file)\n",
    "        #rules\n",
    "        #rule 2-> if none of the elements in the row is NULL and not just numerics\n",
    "        # selected_idx_rows = \n",
    "        # print(df_feature_list)\n",
    "        # print(file)\n",
    "        # model.fit(feature_list_vector)\n",
    "        # y_pred_train = model.predict(feature_list_vector)\n",
    "        # print('y_pred_train', y_pred_train)\n",
    "        # kmeans.fit(y_pred_train.reshape(-1,1))\n",
    "        # identified_clusters = kmeans.fit_predict(y_pred_train.reshape(-1,1))\n",
    "        # print('cluster', identified_clusters)\n",
    "        \n",
    "    else:\n",
    "        print('no header present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554d52d-24a3-46d9-bdc8-836a83168c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = 'data/0ee50c27-22c9-4323-b41e-3f9f71257ba1.txt'\n",
    "Features().rule_based_classification(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c65e3219-52ab-4840-9240-b9563ef2d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_headers(csv_file_name):\n",
    "    df = pd.read_csv(csv_file_name)\n",
    "    file_names = df['file_name']\n",
    "    headers = df['has_header']\n",
    "    for file_name, header in zip(file_names, headers):\n",
    "        if header == 'yes':\n",
    "            file = open('data/'+file_name, 'rb')\n",
    "            lines = file.readlines()\n",
    "            print(lines[0])\n",
    "        else:\n",
    "            print('no header present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c08b6d-8d03-4a79-a54e-6b48d054f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_headers('header_information.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b5318-729e-449b-8dd7-96fb8d8164b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
